import os
import json
import subprocess


def get_git_diff():
    result = subprocess.run(
        ["git", "diff", "HEAD~1"],
        capture_output=True,
        text=True
    )
    return result.stdout


def load_json_file(file_path):
    if not os.path.exists(file_path):
        return {}
    with open(file_path, "r") as f:
        return json.load(f)


def mock_ai_analysis(diff):

    vulnerabilities = []

    # Hardcoded credentials detection
    if 'password == "' in diff or 'username == "' in diff:
        vulnerabilities.append({
            "type": "Hardcoded Credential",
            "cwe": "CWE-798",
            "severity": "High",
            "confidence": 0.9,
            "explanation": "Hardcoded credentials detected.",
            "secure_fix": "Use environment variables."
        })

    # Command execution detection
    if "os.system(" in diff:
        vulnerabilities.append({
            "type": "Command Injection",
            "cwe": "CWE-78",
            "severity": "Critical",
            "confidence": 0.95,
            "explanation": "Unsafe system command execution.",
            "secure_fix": "Avoid os.system; validate inputs."
        })

    return {"vulnerabilities": vulnerabilities}


def calculate_risk(severity, confidence):

    weights = {
        "Low": 1,
        "Medium": 2,
        "High": 3,
        "Critical": 4
    }

    return weights.get(severity, 1) * confidence


def generate_markdown_summary(result):
    
    total_vulns = len(result["vulnerabilities"])
    
    md = f"## ðŸ›¡ï¸ AI Security Analysis Report\n\n"
    
    if total_vulns == 0:
        md += "âœ… **No high-risk vulnerabilities detected by AI.**\n\n"
        md += "> The Mock AI analyzer reviewed the latest changes and found no issues.\n"
        return md
        
    md += f"âš ï¸ **Found {total_vulns} potential security issue(s)**\n\n"
    
    md += "| Type | Severity | CWE | Explanation | Fix |\n"
    md += "|---|---|---|---|---|\n"
    
    for v in result["vulnerabilities"]:
        md += f"| **{v['type']}** | {v['severity']} | {v['cwe']} | {v['explanation']} | `{v['secure_fix']}` |\n"
        
    md += "\n> Note: This report was generated by the Mock AI Security Analyzer.\n"
    return md


if __name__ == "__main__":

    diff = get_git_diff()

    if not diff.strip():
        print("No changes detected.")
        exit(0)

    result = mock_ai_analysis(diff)

    print(json.dumps(result, indent=2))
    
    # Save for dashboard artifact
    with open("mock-ai-report.json", "w") as f:
        json.dump(result, f, indent=2)

    # Write to GitHub Step Summary
    summary_file = os.environ.get("GITHUB_STEP_SUMMARY")
    if summary_file:
        with open(summary_file, "a") as f:
            f.write(generate_markdown_summary(result))

    # Adaptive enforcement gate
    blocked = False
    for v in result["vulnerabilities"]:
        risk = calculate_risk(v["severity"], v["confidence"])
        print(f"Risk Score: {risk}")

        if risk >= 3:
            blocked = True
            
    if blocked:
        print("ðŸš¨ Mock AI Security Gate BLOCKED deployment")
        exit(1)
